{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf144ea-ce4e-4d07-bd72-9a6839abf027",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Part-of-Speech (POS) tagging is an essential task in Natural Language Processing (NLP) that involves labeling words in a sentence with their corresponding grammatical roles, such as noun, verb, adjective, etc. Understanding the structure of a sentence through POS tags is crucial for various downstream tasks, including text analysis, syntactic parsing, and language modeling.\n",
    "\n",
    "In this project, we aim to build a model that can accurately identify the parts of speech in a sentence. Using the CoNLL-2003 dataset, we will not only recognize the basic POS tags but also explore more complex structures such as noun phrases (NP) and verb phrases (VP). The goal is to demonstrate how POS tagging can be leveraged to gain deeper insights into sentence structure and meaning.\n",
    "\n",
    "This project is structured as follows:\n",
    "1. **Data Preparation**: Loading and exploring the CoNLL-2003 dataset, and preprocessing it to extract POS tags and phrase structures.\n",
    "2. **Model Training**: Building a model that identifies POS tags and phrase structures using advanced NLP techniques.\n",
    "3. **Evaluation and Analysis**: Evaluating the model's performance in identifying POS tags and analyzing its predictions.\n",
    "\n",
    "By the end of this project, you will have a clear understanding of how to implement a POS tagging model and how it can be applied to various NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b271da",
   "metadata": {},
   "source": [
    "### Preparing the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce973e0",
   "metadata": {},
   "source": [
    "##### The CoNLL-2003 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a42be30-bf4e-4201-9638-fef8eadabcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fe0cc4e-c9ee-43c3-9b52-d453834b8238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4894b1df-835c-4626-960a-6a738d57c9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "212509ba-df77-4c68-bc31-ed028fdc5d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 21, 11, 12, 21, 22, 11, 12, 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]['chunk_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f37fac2-8f7e-4f02-8d57-2450bd2ddebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features['chunk_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71bd32fb-897e-4c58-b7ad-c7da9e50dd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ADJP',\n",
       " 'I-ADJP',\n",
       " 'B-ADVP',\n",
       " 'I-ADVP',\n",
       " 'B-CONJP',\n",
       " 'I-CONJP',\n",
       " 'B-INTJ',\n",
       " 'I-INTJ',\n",
       " 'B-LST',\n",
       " 'I-LST',\n",
       " 'B-NP',\n",
       " 'I-NP',\n",
       " 'B-PP',\n",
       " 'I-PP',\n",
       " 'B-PRT',\n",
       " 'I-PRT',\n",
       " 'B-SBAR',\n",
       " 'I-SBAR',\n",
       " 'B-UCP',\n",
       " 'I-UCP',\n",
       " 'B-VP',\n",
       " 'I-VP']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_feature = raw_datasets['train'].features['chunk_tags']\n",
    "label_names = chunk_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93c644de-c0cd-45b9-b114-c0f73ea45553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU   rejects German call to   boycott British lamb . \n",
      "B-NP B-VP    B-NP   I-NP B-VP I-VP    B-NP    I-NP O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"chunk_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd2013-1cff-4546-8eb1-259624fd2509",
   "metadata": {},
   "source": [
    "### Data Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95926e19-359a-4cc4-838d-fa5be361c32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = 'distilbert/distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9121a2c-ff2b-4ef7-abcd-da0554c70d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2de97649-201b-43f5-9327-4f9ba4bc0ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets['train'][0]['tokens'], is_split_into_words = True)\n",
    "inputs.tokens()\n",
    "#the tokenizer added the special characters and extra tokens\n",
    "#To counter this we use word_ids to get the labels for all added tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b884cd2-fd50-4d85-aaf0-6040fc8b3ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c3b089d-1afd-47d9-91eb-3b123f044dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are adjusting new labels based on word ids\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id] \n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85e27342-8157-49dd-8a0c-3e1b6550249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 21, 11, 12, 21, 22, 11, 12, 0]\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n",
      "[-100, 11, 21, 11, 12, 21, 22, 11, 12, 12, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets['train'][0]['chunk_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(word_ids)\n",
    "print(align_labels_with_tokens(labels,word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8c7c5c1-475d-4c10-b8f9-869f28f406f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to tokenize the whole dataset \n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example['tokens'], truncation = True, is_split_into_words = True)\n",
    "    all_labels = example['chunk_tags'] #it is a list of lists because we used batched = True in map method\n",
    "    new_labels = []\n",
    "    for i,labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels,word_ids))\n",
    "\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d48b98e-0057-4f4d-9af3-57238db4ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched = True, remove_columns=raw_datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04b2979f-66cf-4a2b-a5e6-3cccb23b656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca32165-7c61-47f8-8726-58aa1a2d63fc",
   "metadata": {},
   "source": [
    "#### Fine tuning the model with Trainer API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5641c53c",
   "metadata": {},
   "source": [
    "##### Data Collation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "363240bf-2c65-46d8-aa71-4195730c1a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c461dd8e-0c90-4f95-bfdc-0c01ce9c9977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-100,   11,   21,   11,   12,   21,   22,   11,   12,   12,    0, -100],\n",
      "        [-100,   11,   12, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "print(batch['labels'])\n",
    "#we are adding -100 to make sure that the padded tokens are not considered while computing the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac2e6b-1fdc-4902-8e88-ad9b7bf4b923",
   "metadata": {},
   "source": [
    "##### Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a1fc3f7-9982-4974-a199-6c633b95030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('seqeval') #it needs the strings not the integers to compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c8f7d4e-1d8b-41f5-b0b2-6cc29ad767ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'B-NP', 'I-NP', 'O']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets['train'][0]['chunk_tags']\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7ae424f-9eda-4f66-a09e-e40abaa913a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'VP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.2222222222222222}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = ['O','O','O','O','O','O','O','O','O']\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bbe8282c-3168-466b-bde9-93cb8c967bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ADJP',\n",
       " 'I-ADJP',\n",
       " 'B-ADVP',\n",
       " 'I-ADVP',\n",
       " 'B-CONJP',\n",
       " 'I-CONJP',\n",
       " 'B-INTJ',\n",
       " 'I-INTJ',\n",
       " 'B-LST',\n",
       " 'I-LST',\n",
       " 'B-NP',\n",
       " 'I-NP',\n",
       " 'B-PP',\n",
       " 'I-PP',\n",
       " 'B-PRT',\n",
       " 'I-PRT',\n",
       " 'B-SBAR',\n",
       " 'I-SBAR',\n",
       " 'B-UCP',\n",
       " 'I-UCP',\n",
       " 'B-VP',\n",
       " 'I-VP']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f4045262-38d3-494a-9252-8beec91669c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits,labels = eval_preds\n",
    "    predictions = np.argmax(logits,axis = -1)\n",
    "    true_labels = [[label_names[l] for l in label if l!=-100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for (p,l) in zip(prediction,label) if l!=-100] for (prediction,label) in zip(predictions,labels)]\n",
    "    all_metrics = metric.compute(predictions = true_predictions, references = true_labels)\n",
    "    return {\n",
    "    'precision': all_metrics['overall_precision'],\n",
    "    'recall': all_metrics['overall_recall'],\n",
    "    'f1': all_metrics['overall_f1'],\n",
    "    'accuracy': all_metrics['overall_accuracy']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c3199-e5bd-43f4-bc9c-252a7d800ed3",
   "metadata": {},
   "source": [
    "##### Defining the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4fd5df9e-df86-4cd7-b643-4343b63d4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i:label for i,label in enumerate(label_names)}\n",
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb0344d5-236a-40e7-965f-b9fd341b5c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469703e562ec4ef287b92cf54e46f0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, id2label=id2label, label2id = label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "379499da-bae8-4e26-847d-443cafa047b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db2a7d",
   "metadata": {},
   "source": [
    "##### Fine-tuning the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e139cd14-4464-4bb6-84e7-34501dfa0107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578783d4afdb47e7b78bc2f94dec405c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c84f10c1-9092-4e47-a919-1d10284938f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments('bert-finetuned-ner',\n",
    "                         evaluation_strategy = 'epoch',\n",
    "                         save_strategy = 'epoch',\n",
    "                         learning_rate = 2e-5,\n",
    "                         num_train_epochs = 3,\n",
    "                         weight_decay = 0.01,\n",
    "                         push_to_hub = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8308dbfc-19e7-4a1e-81f8-0ce1ec0b1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model = model,\n",
    "                  args = args,\n",
    "                  train_dataset = tokenized_datasets['train'],\n",
    "                  eval_dataset = tokenized_datasets['validation'],\n",
    "                  data_collator = data_collator,\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f0fbcc3d-ff41-4f1d-a03b-c4dfa39303c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 1:49:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.192600</td>\n",
       "      <td>0.180864</td>\n",
       "      <td>0.910433</td>\n",
       "      <td>0.905622</td>\n",
       "      <td>0.908021</td>\n",
       "      <td>0.954303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.162188</td>\n",
       "      <td>0.920037</td>\n",
       "      <td>0.915569</td>\n",
       "      <td>0.917798</td>\n",
       "      <td>0.959160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.163975</td>\n",
       "      <td>0.922300</td>\n",
       "      <td>0.919182</td>\n",
       "      <td>0.920739</td>\n",
       "      <td>0.960676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.17121758203933704, metrics={'train_runtime': 6553.4701, 'train_samples_per_second': 6.428, 'train_steps_per_second': 0.804, 'total_flos': 460548101514270.0, 'train_loss': 0.17121758203933704, 'epoch': 3.0})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9b547fc0-cd68-42d4-b691-d6f0ca43f86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Tarun-1999M/bert-finetuned-ner/commit/49ae5c0909330944418a5c663e5fe89f70d3adc3', commit_message='Training complete', commit_description='', oid='49ae5c0909330944418a5c663e5fe89f70d3adc3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624b1cd",
   "metadata": {},
   "source": [
    "### Using the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a34bbbf-1372-40af-ae5b-09ab58195478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'NP',\n",
       "  'score': 0.9982133,\n",
       "  'word': 'My name',\n",
       "  'start': 0,\n",
       "  'end': 7},\n",
       " {'entity_group': 'VP',\n",
       "  'score': 0.9985273,\n",
       "  'word': 'is',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity_group': 'NP',\n",
       "  'score': 0.98740447,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'NP',\n",
       "  'score': 0.9938041,\n",
       "  'word': 'I',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity_group': 'VP',\n",
       "  'score': 0.9956542,\n",
       "  'word': 'work',\n",
       "  'start': 25,\n",
       "  'end': 29},\n",
       " {'entity_group': 'PP',\n",
       "  'score': 0.9982097,\n",
       "  'word': 'at',\n",
       "  'start': 30,\n",
       "  'end': 32},\n",
       " {'entity_group': 'NP',\n",
       "  'score': 0.9252255,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'PP',\n",
       "  'score': 0.99902964,\n",
       "  'word': 'in',\n",
       "  'start': 46,\n",
       "  'end': 48},\n",
       " {'entity_group': 'NP',\n",
       "  'score': 0.9994374,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Tarun-1999M/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32a755ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp NER_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afae102-ca95-4392-93a5-268286d550c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "# Import necessary libraries\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned BERT model for token classification\n",
    "model_checkpoint = \"Tarun-1999M/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Set up the Gradio interface\n",
    "title = \"Token Classification with Fine-tuned DISTILBERT\"\n",
    "description = \"\"\"\n",
    "This application identifies and classifies tokens (e.g., named entities) in a given text using a DISTILBERT model fine-tuned for NER. \n",
    "Input any text to see how the model labels the tokens.\n",
    "\n",
    "### Explanation of Abbreviations:\n",
    "- **O**: Outside of a named entity\n",
    "- **ADJP**: Adjective Phrase\n",
    "- **ADVP**: Adverb Phrase\n",
    "- **CONJP**: Conjunction Phrase\n",
    "- **INTJ**: Interjection\n",
    "- **LST**: List Item Marker\n",
    "- **NP**: Noun Phrase\n",
    "- **PP**: Prepositional Phrase\n",
    "- **PRT**: Particle\n",
    "- **SBAR**: Subordinating Conjunction Clause\n",
    "- **UCP**: Unlike Coordinated Phrase\n",
    "- **VP**: Verb Phrase\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "article = \"This demo uses a DISTILBERT model fine-tuned on a specific task for token classification.\"\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(text):\n",
    "    results = token_classifier(text)\n",
    "    return results\n",
    "\n",
    "# Gradio interface\n",
    "gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"json\",\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article,\n",
    "    examples=[\n",
    "        [\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"],\n",
    "        [\"Albert Einstein was a physicist and he developed the theory of relativity.\"],\n",
    "        [\"Python is a programming language that I use daily.\"]\n",
    "    ],\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4111d953-30b1-4bc0-ac49-30a7ad413520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "notebook_name = \"NER_POS_tagging.ipynb\"\n",
    "export_destination = \".\"\n",
    "nbdev.export.nb_export(notebook_name, export_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb08cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44082c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a101a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7463d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
